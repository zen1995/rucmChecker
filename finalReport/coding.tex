\chapter{代码实现}

\section{相关技术}
    \subsection{NLP}
    \subsection{BFS}
\section{开发协作流程}
    \subsection{GIT}
    \subsection{分模块开发}
\section{问题与解决办法}
    \subsection{NLP Server}
    \subsection{复杂规则的实现}
    \subsection{中文适配}

    在第一次的展示中，吴老师给出要增加可以处理rucm中出现中文的情况，并检查其他组的rucm文件作为联系。
    增加对中文的适配，难度主要有2点：
    
    \begin{itemize}
        \item RUCM manual中，所有26条规则的定义和示例都是针对英文的，完全没有考虑中文。比如，对于第15条规则，中文中并不存在分词。所以我们需要重新定义26条默认规则，删除不必要和不方便实现的默认规则。
        \item 中文NLP技术与英文的差别还是很大的，尤其是语法分析部分，句子成分和词性区别很大。很多函数需要准备2套规则，分别适应于不同的语言。对于一个句子中可能同时出现中英文的情况，比如有些同学的rucm文件的关键字和名词用英文，其他地方使用中文，对于这种情况，我们经过讨论决定，不予支持。即不允许中英文在一个句子里混杂，但是允许不同句子使用不同的语言。
    \end{itemize}

    中文NLP主要借助了2个工具：结巴分词 和 Standford parser for Chinese。
    其中，结巴分词主要用于，对中文进行分词，和词性分析。
    Standford parser for Chinese用于中文的语法分析。
    工具的选择主要考虑到开发速度、易用性和准确性。英文中不存在分词问题，每个单词都是以空格分隔的。但中文中，分词确实被广泛研究的，是一切NLP的基础。经过调研，“结巴分词”使用率最广。我们选择使用“结巴分词”作为我们的分词工具和词性分析工具。
    结巴分词并不提供更高级的NLP技术。对于语法分析，我们仍然选择Stanford 的parser。主要考虑是，Standford parser for Chinese的接口和使用方法与Stanford parser for English相同。直接使用它可以降低学习成本，提高开发效率。

    针对可能出现的中英文混杂，我们实现了自动切换分析工具的功能。即根据句子的语言类别，使用2套不同的函数进行分析、NLP处理。降低了用户使用的难度，提高了可用性。
    